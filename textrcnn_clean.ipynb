{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DistilBERT + TextRCNN Pipeline\n",
        "## Advanced Document Classification with Full Parameter Finetuning\n",
        "\n",
        "This notebook implements DistilBERT + TextRCNN architecture for document authenticity classification kaggle competition Fake or Real: The Impostor Hunt in Texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using MPS (Metal GPU) device\n",
            "Device: mps\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print('Using MPS (Metal GPU) device')\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('Using CUDA device')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('Using CPU device')\n",
        "\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TextRCNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextRCNN(nn.Module):\n",
        "    \"\"\"TextRCNN: RNN + CNN + Attention for text classification.\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size=768, num_layers=2, num_classes=2, dropout=0.3):\n",
        "        super(TextRCNN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size // 2,  # Bidirectional will double this\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        # CNN layers for local feature extraction\n",
        "        self.conv1 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
        "        \n",
        "        # Attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=8,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),  # LSTM output is already hidden_size\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, bert_outputs, attention_mask=None):\n",
        "        # bert_outputs: [batch_size, seq_len, hidden_size]\n",
        "        batch_size, seq_len, hidden_size = bert_outputs.shape\n",
        "        \n",
        "        # Bidirectional LSTM\n",
        "        lstm_out, _ = self.lstm(bert_outputs)\n",
        "        # lstm_out: [batch_size, seq_len, hidden_size]\n",
        "        \n",
        "        # CNN feature extraction\n",
        "        # Transpose for CNN: [batch_size, hidden_size, seq_len]\n",
        "        cnn_input = lstm_out.transpose(1, 2)\n",
        "        \n",
        "        # Apply CNN layers with residual connections\n",
        "        conv1_out = F.relu(self.conv1(cnn_input))\n",
        "        conv2_out = F.relu(self.conv2(conv1_out))\n",
        "        conv3_out = F.relu(self.conv3(conv2_out))\n",
        "        \n",
        "        # Transpose back: [batch_size, seq_len, hidden_size]\n",
        "        cnn_out = conv3_out.transpose(1, 2)\n",
        "        \n",
        "        # Add residual connection\n",
        "        cnn_out = self.layer_norm1(cnn_out + lstm_out)\n",
        "        \n",
        "        # Selfattention mechanism (simplified to avoid mask issues)\n",
        "        # Use simple attention without complex masking\n",
        "        attn_out, _ = self.attention(cnn_out, cnn_out, cnn_out)\n",
        "        \n",
        "        # Add residual connection\n",
        "        attn_out = self.layer_norm2(attn_out + cnn_out)\n",
        "        \n",
        "        # Global average pooling\n",
        "        if attention_mask is not None:\n",
        "            # Masked average pooling\n",
        "            masked_output = attn_out * attention_mask.unsqueeze(-1)\n",
        "            pooled_output = masked_output.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
        "        else:\n",
        "            pooled_output = attn_out.mean(dim=1)\n",
        "        \n",
        "        # Classification\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete DistilBERT + TextRCNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DistilBertTextRCnnClassifier(nn.Module):\n",
        "    \"\"\"Complete DistilBERT + TextRCNN classifier.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name='distilbertbaseuncased', num_classes=2, dropout=0.3):\n",
        "        super(DistilBertTextRCnnClassifier, self).__init__()\n",
        "        \n",
        "        # DistilBERT encoder\n",
        "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
        "        \n",
        "        # TextRCNN classifier\n",
        "        self.textrcnn = TextRCNN(\n",
        "            hidden_size=768,  # DistilBERT hidden size\n",
        "            num_layers=2,\n",
        "            num_classes=num_classes,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # Freeze BERT initially (will unfreeze during finetuning)\n",
        "        self.freeze_bert()\n",
        "        \n",
        "    def freeze_bert(self):\n",
        "        \"\"\"Freeze BERT parameters.\"\"\"\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"BERT parameters frozen\")\n",
        "    \n",
        "    def unfreeze_bert(self):\n",
        "        \"\"\"Unfreeze BERT parameters for finetuning.\"\"\"\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(\"BERT parameters unfrozen for finetuning\")\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get BERT outputs\n",
        "        bert_outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ).last_hidden_state\n",
        "        \n",
        "        # Pass through TextRCNN\n",
        "        logits = self.textrcnn(bert_outputs, attention_mask)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocumentDataset(Dataset):\n",
        "    \"\"\"Custom dataset for document classification.\"\"\"\n",
        "    \n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item['text']\n",
        "        label = item['label']\n",
        "        \n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_document_pairs(data_dir):\n",
        "    \"\"\"Load document pairs from the data directory.\"\"\"\n",
        "    pairs = []\n",
        "    \n",
        "    for article_dir in sorted(os.listdir(data_dir)):\n",
        "        article_path = os.path.join(data_dir, article_dir)\n",
        "        if os.path.isdir(article_path):\n",
        "            files = [f for f in os.listdir(article_path) if f.endswith('.txt')]\n",
        "            if len(files) == 2:\n",
        "                file1_path = os.path.join(article_path, files[0])\n",
        "                file2_path = os.path.join(article_path, files[1])\n",
        "                \n",
        "                with open(file1_path, 'r', encoding='utf8') as f:\n",
        "                    content1 = f.read().strip()\n",
        "                with open(file2_path, 'r', encoding='utf8') as f:\n",
        "                    content2 = f.read().strip()\n",
        "                \n",
        "                pairs.append({\n",
        "                    'article_id': article_dir,\n",
        "                    'file1': files[0],\n",
        "                    'file2': files[1],\n",
        "                    'content1': content1,\n",
        "                    'content2': content2,\n",
        "                    'file1_path': file1_path,\n",
        "                    'file2_path': file2_path\n",
        "                })\n",
        "    \n",
        "    return pairs\n",
        "\n",
        "def create_training_data(pairs, labels_df):\n",
        "    \"\"\"Create training data with one text per row and proper labels.\"\"\"\n",
        "    \n",
        "    print(f\"Creating training data with {len(labels_df)} articles...\")\n",
        "    \n",
        "    training_data = []\n",
        "    \n",
        "    for _, row in labels_df.iterrows():\n",
        "        article_id = row['id']\n",
        "        real_text_id = row['real_text_id']\n",
        "        \n",
        "        article_folder = f\"article_{str(article_id).zfill(4)}\"\n",
        "        pair = None\n",
        "        \n",
        "        for p in pairs:\n",
        "            if p['article_id'] == article_folder:\n",
        "                pair = p\n",
        "                break\n",
        "        \n",
        "        if pair is None:\n",
        "            print(f\"Warning: Could not find pair for article {article_id}\")\n",
        "            continue\n",
        "        \n",
        "        if real_text_id == 1:\n",
        "            real_content = pair['content1']\n",
        "            fake_content = pair['content2']\n",
        "        else:\n",
        "            real_content = pair['content2']\n",
        "            fake_content = pair['content1']\n",
        "        \n",
        "        training_data.append({\n",
        "            'text': real_content,\n",
        "            'label': 1,\n",
        "            'article_id': article_id,\n",
        "            'text_type': 'real'\n",
        "        })\n",
        "        \n",
        "        training_data.append({\n",
        "            'text': fake_content,\n",
        "            'label': 0,\n",
        "            'article_id': article_id,\n",
        "            'text_type': 'fake'\n",
        "        })\n",
        "    \n",
        "    print(f\"Created {len(training_data)} training examples\")\n",
        "    print(f\"    Real documents: {len([x for x in training_data if x['label'] == 1])}\")\n",
        "    print(f\"    Fake documents: {len([x for x in training_data if x['label'] == 0])}\")\n",
        "    \n",
        "    return training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=2e-5):\n",
        "    \"\"\"Train the DistilBERT + TextRCNN model with improved validation.\"\"\"\n",
        "    \n",
        "    print(\" Training DistilBERT + TextRCNN model...\")\n",
        "    \n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "    \n",
        "    # Learning rate scheduler\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "    \n",
        "    # Training loop\n",
        "    best_val_acc = 0\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        train_acc = correct / total\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        \n",
        "        # Validation phase with better debugging\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_loss = 0\n",
        "        val_predictions = []\n",
        "        val_true_labels = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "                \n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                \n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                \n",
        "                # Store predictions for debugging\n",
        "                val_predictions.extend(predicted.cpu().numpy())\n",
        "                val_true_labels.extend(labels.cpu().numpy())\n",
        "        \n",
        "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
        "        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"  Training Loss: {avg_loss:.4f}, Training Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Validation Loss: {avg_val_loss:.4f}, Validation Acc: {val_acc:.4f}\")\n",
        "        \n",
        "        # Debug validation predictions\n",
        "        if epoch == 0 or val_acc == 0:\n",
        "            print(f\"   Validation Debug:\")\n",
        "            print(f\"      Total validation samples: {val_total}\")\n",
        "            print(f\"      Correct predictions: {val_correct}\")\n",
        "            print(f\"      Prediction distribution: {np.bincount(val_predictions) if len(val_predictions) > 0 else 'No predictions'}\")\n",
        "            print(f\"      True label distribution: {np.bincount(val_true_labels) if len(val_true_labels) > 0 else 'No labels'}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_distilbert_textrcnn_model.pth')\n",
        "            print(f\"   New best validation accuracy: {best_val_acc:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"    Early stopping after {patience} epochs without improvement\")\n",
        "            break\n",
        "        \n",
        "        # Unfreeze BERT after first few epochs for finetuning\n",
        "        if epoch == 2:\n",
        "            model.unfreeze_bert()\n",
        "            print(\"   Unfrozen BERT for finetuning\")\n",
        "    \n",
        "    print(f\"\\n Best validation accuracy: {best_val_acc:.4f}\")\n",
        "    print(\" Training completed!\")\n",
        "    \n",
        "    return best_val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Validation Debugging\n",
        "\n",
        "This cell helps debug validation issues if they occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation debugging helper\n",
        "def debug_validation(model, val_loader, device):\n",
        "    \"\"\"Debug validation issues.\"\"\"\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "    \n",
        "    print(\" Validation Debugging...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            if i >= 3:  # Only check first 3 batches\n",
        "                break\n",
        "                \n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            print(f\"\\nBatch {i+1}:\")\n",
        "            print(f\"  Input shape: {input_ids.shape}\")\n",
        "            print(f\"  Labels: {labels}\")\n",
        "            \n",
        "            try:\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                print(f\"  Output shape: {outputs.shape}\")\n",
        "                print(f\"  Output values: {outputs[0]}\")\n",
        "                \n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                print(f\"  Predictions: {predicted}\")\n",
        "                \n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                \n",
        "                val_predictions.extend(predicted.cpu().numpy())\n",
        "                val_true_labels.extend(labels.cpu().numpy())\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   Error: {e}\")\n",
        "    \n",
        "    print(f\"\\n Summary:\")\n",
        "    print(f\"  Total samples checked: {val_total}\")\n",
        "    print(f\"  Correct predictions: {val_correct}\")\n",
        "    print(f\"  Prediction distribution: {np.bincount(val_predictions) if len(val_predictions) > 0 else 'No predictions'}\")\n",
        "    print(f\"  True label distribution: {np.bincount(val_true_labels) if len(val_true_labels) > 0 else 'No labels'}\")\n",
        "    \n",
        "    if val_total > 0:\n",
        "        accuracy = val_correct / val_total\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    \n",
        "    return val_predictions, val_true_labels\n",
        "\n",
        "# You can call this function if validation fails:\n",
        "# debug_validation(model, val_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_test_set(model, tokenizer, test_pairs, max_length=512):\n",
        "    \"\"\"Generate predictions on test set.\"\"\"\n",
        "    \n",
        "    print(\" Generating test predictions...\")\n",
        "    \n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    \n",
        "    for i, pair in enumerate(test_pairs):\n",
        "        article_id = pair['article_id']\n",
        "        try:\n",
        "            numeric_id = int(article_id.split('_')[1])\n",
        "            solution_id = numeric_id\n",
        "        except (IndexError, ValueError):\n",
        "            solution_id = i\n",
        "        \n",
        "        text1 = pair['content1']\n",
        "        text2 = pair['content2']\n",
        "        \n",
        "        # Tokenize texts\n",
        "        encoding1 = tokenizer(\n",
        "            text1,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        encoding2 = tokenizer(\n",
        "            text2,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Move to device\n",
        "        input_ids1 = encoding1['input_ids'].to(device)\n",
        "        attention_mask1 = encoding1['attention_mask'].to(device)\n",
        "        input_ids2 = encoding2['input_ids'].to(device)\n",
        "        attention_mask2 = encoding2['attention_mask'].to(device)\n",
        "        \n",
        "        # Get predictions\n",
        "        with torch.no_grad():\n",
        "            outputs1 = model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
        "            outputs2 = model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
        "            \n",
        "            probs1 = F.softmax(outputs1, dim=1)\n",
        "            probs2 = F.softmax(outputs2, dim=1)\n",
        "            \n",
        "            pred1 = torch.argmax(outputs1, dim=1).item()\n",
        "            pred2 = torch.argmax(outputs2, dim=1).item()\n",
        "            \n",
        "            real_prob1 = probs1[0][1].item()\n",
        "            real_prob2 = probs2[0][1].item()\n",
        "        \n",
        "        # Determine which file is real\n",
        "        if pred1 == 1 and pred2 == 0:\n",
        "            real_text_id = 1\n",
        "        elif pred1 == 0 and pred2 == 1:\n",
        "            real_text_id = 2\n",
        "        else:\n",
        "            # Use probability\n",
        "            real_text_id = 1 if real_prob1 > real_prob2 else 2\n",
        "        \n",
        "        predictions.append({\n",
        "            'id': solution_id,\n",
        "            'real_text_id': real_text_id,\n",
        "            'text1_pred': pred1,\n",
        "            'text2_pred': pred2,\n",
        "            'text1_real_prob': real_prob1,\n",
        "            'text2_real_prob': real_prob2\n",
        "        })\n",
        "        \n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Processed {i + 1}/{len(test_pairs)} pairs...\")\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Pipeline Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Step 1: Loading data...\n",
            "Loaded 95 training articles\n",
            "Loaded 95 training pairs\n",
            "Loaded 1068 test pairs\n"
          ]
        }
      ],
      "source": [
        "# 1. Load data\n",
        "print(\" Step 1: Loading data...\")\n",
        "labels_df = pd.read_csv('train.csv')\n",
        "train_pairs = load_document_pairs('train')\n",
        "test_pairs = load_document_pairs('test')\n",
        "\n",
        "print(f\"Loaded {len(labels_df)} training articles\")\n",
        "print(f\"Loaded {len(train_pairs)} training pairs\")\n",
        "print(f\"Loaded {len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Step 2: Creating training data...\n",
            "Creating training data with 95 articles...\n",
            "Created 190 training examples\n",
            "    Real documents: 95\n",
            "    Fake documents: 95\n"
          ]
        }
      ],
      "source": [
        "# 2. Create training data\n",
        "print(\" Step 2: Creating training data...\")\n",
        "train_data = create_training_data(train_pairs, labels_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3: Initializing DistilBERT + TextRCNN...\n",
            "BERT parameters frozen\n",
            "Model initialized on mps\n",
            "Total parameters: 82,015,874\n",
            "Trainable parameters: 15,652,994\n"
          ]
        }
      ],
      "source": [
        "# 3. Initialize tokenizer and model\n",
        "print(\"Step 3: Initializing DistilBERT + TextRCNN...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertTextRCnnClassifier(\n",
        "    model_name='distilbert-base-uncased',\n",
        "    num_classes=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model initialized on {device}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4: Creating datasets...\n",
            "Training batches: 19\n",
            "Validation batches: 5\n"
          ]
        }
      ],
      "source": [
        "# 4. Create datasets and data loaders\n",
        "print(\"Step 4: Creating datasets...\")\n",
        "\n",
        "# Create DocumentDataset instances\n",
        "train_dataset = DocumentDataset(train_data, tokenizer)\n",
        "val_dataset = DocumentDataset(train_data, tokenizer)  # Use full data for validation\n",
        "\n",
        "# Split the dataset\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    train_dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 8  # Adjust based on your GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Step 5: Training model...\n",
            " Training DistilBERT + TextRCNN model...\n",
            "Epoch 1/15:\n",
            "  Training Loss: 0.6889, Training Acc: 0.5724\n",
            "  Validation Loss: 0.6652, Validation Acc: 0.7368\n",
            "   Validation Debug:\n",
            "      Total validation samples: 38\n",
            "      Correct predictions: 28\n",
            "      Prediction distribution: [ 7 31]\n",
            "      True label distribution: [17 21]\n",
            "   New best validation accuracy: 0.7368\n",
            "Epoch 2/15:\n",
            "  Training Loss: 0.6438, Training Acc: 0.7039\n",
            "  Validation Loss: 0.6128, Validation Acc: 0.7632\n",
            "   New best validation accuracy: 0.7632\n",
            "Epoch 3/15:\n",
            "  Training Loss: 0.5896, Training Acc: 0.7697\n",
            "  Validation Loss: 0.5554, Validation Acc: 0.7632\n",
            "BERT parameters unfrozen for finetuning\n",
            "   Unfrozen BERT for finetuning\n",
            "Epoch 4/15:\n",
            "  Training Loss: 0.4989, Training Acc: 0.8026\n",
            "  Validation Loss: 0.4497, Validation Acc: 0.7895\n",
            "   New best validation accuracy: 0.7895\n",
            "Epoch 5/15:\n",
            "  Training Loss: 0.3891, Training Acc: 0.8487\n",
            "  Validation Loss: 0.3905, Validation Acc: 0.7895\n",
            "Epoch 6/15:\n",
            "  Training Loss: 0.2755, Training Acc: 0.8816\n",
            "  Validation Loss: 0.4508, Validation Acc: 0.8421\n",
            "   New best validation accuracy: 0.8421\n",
            "Epoch 7/15:\n",
            "  Training Loss: 0.1802, Training Acc: 0.9408\n",
            "  Validation Loss: 0.4561, Validation Acc: 0.7632\n",
            "Epoch 8/15:\n",
            "  Training Loss: 0.3070, Training Acc: 0.8816\n",
            "  Validation Loss: 0.5305, Validation Acc: 0.8421\n",
            "Epoch 9/15:\n",
            "  Training Loss: 0.1453, Training Acc: 0.9474\n",
            "  Validation Loss: 0.5969, Validation Acc: 0.8158\n",
            "Epoch 10/15:\n",
            "  Training Loss: 0.1032, Training Acc: 0.9605\n",
            "  Validation Loss: 0.7480, Validation Acc: 0.8158\n",
            "Epoch 11/15:\n",
            "  Training Loss: 0.1149, Training Acc: 0.9474\n",
            "  Validation Loss: 0.6334, Validation Acc: 0.8158\n",
            "    Early stopping after 5 epochs without improvement\n",
            "\n",
            " Best validation accuracy: 0.8421\n",
            " Training completed!\n",
            " Training completed successfully! Best validation accuracy: 0.8421\n"
          ]
        }
      ],
      "source": [
        "# 5. Train the model\n",
        "print(\" Step 5: Training model...\")\n",
        "try:\n",
        "    best_val_acc = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=15,\n",
        "        learning_rate=2e-5\n",
        "    )\n",
        "    print(f\" Training completed successfully! Best validation accuracy: {best_val_acc:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\" Training failed: {e}\")\n",
        "    print(\" Running validation debug...\")\n",
        "    try:\n",
        "        debug_validation(model, val_loader, device)\n",
        "    except:\n",
        "        print(\"  Debug function not available\")\n",
        "    best_val_acc = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 6: Loading best model\n",
            "Best model loaded\n"
          ]
        }
      ],
      "source": [
        "# 6. Load best model\n",
        "print(\"Step 6: Loading best model\")\n",
        "model.load_state_dict(torch.load('best_distilbert_textrcnn_model.pth'))\n",
        "print(\"Best model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Step 7: Generating test predictions...\n",
            " Generating test predictions...\n",
            "Processed 100/1068 pairs...\n",
            "Processed 200/1068 pairs...\n",
            "Processed 300/1068 pairs...\n",
            "Processed 400/1068 pairs...\n",
            "Processed 500/1068 pairs...\n",
            "Processed 600/1068 pairs...\n",
            "Processed 700/1068 pairs...\n",
            "Processed 800/1068 pairs...\n",
            "Processed 900/1068 pairs...\n",
            "Processed 1000/1068 pairs...\n"
          ]
        }
      ],
      "source": [
        "# 7. Generate predictions\n",
        "print(\" Step 7: Generating test predictions...\")\n",
        "predictions = predict_test_set(model, tokenizer, test_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solution file saved as: Hunt_In_Text_Solution.csv\n"
          ]
        }
      ],
      "source": [
        "# 8. Create solution file\n",
        "solution_df = pd.DataFrame(predictions)\n",
        "submission_df = solution_df[['id', 'real_text_id']].copy()\n",
        "\n",
        "submission_df = submission_df.sort_values('id').reset_index(drop=True)\n",
        "submission_df['id'] = submission_df['id'].astype(int)\n",
        "submission_df['real_text_id'] = submission_df['real_text_id'].astype(int)\n",
        "\n",
        "solution_file = 'Hunt_In_Text_Solution.csv'\n",
        "submission_df.to_csv(solution_file, index=False)\n",
        "\n",
        "print(f\"Solution file saved as: {solution_file}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
