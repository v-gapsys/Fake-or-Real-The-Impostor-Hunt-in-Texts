{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved DistilBERT + TextRCNN Pipeline\n",
    "## Advanced Document Classification with Optimized Parameters\n",
    "\n",
    "This notebook implements an improved DistilBERT + TextRCNN architecture for document authenticity classification with better parameters and architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development Journey: From Traditional ML to Advanced Neural Architectures\n",
    "\n",
    "## Initial Approach: Traditional Machine Learning\n",
    "Started with TF-IDF + XGBoost as a baseline approach. This traditional pipeline used:\n",
    "TF-IDF vectorization for text feature extraction\n",
    "XGBoost classifier for document authenticity classification\n",
    "Basic cross-validation and hyperparameter tuning\n",
    "\n",
    "## First Neural Approach: BERT\n",
    "\n",
    "Moved to BERT (Bidirectional Encoder Representations from Transformers) as the primary model:\n",
    "Standard BERT for contextual text understanding\n",
    "Fine-tuning on the document authenticity task\n",
    "Pooled output classification with custom classification heads\n",
    "\n",
    "## Domain-Specific Exploration: SciBERT\n",
    "\n",
    "Experimented with SciBERT (scientific BERT) to leverage domain-specific knowledge:\n",
    "Scientific vocabulary pre-training for better understanding of technical documents\n",
    "Specialized tokenization for scientific and engineering terminology\n",
    "Full fine-tuning approach with unfrozen BERT layers\n",
    "\n",
    "## Ensemble and Pseudo-Labeling Breakthrough\n",
    "\n",
    "Implemented pseudo-labeling techniques that significantly improved results:\n",
    "Random Forest pseudo-labeling for fast, reliable expansion of training data\n",
    "BERT training on expanded dataset using high-confidence predictions\n",
    "\n",
    "## Advanced Architecture: TextRCNN\n",
    "\n",
    "Developed TextRCNN (Text RNN + CNN) architecture combining:\n",
    "Bidirectional LSTM for sequential text processing\n",
    "Multi-scale CNN (3x3, 5x5, 7x7 kernels) for local feature extraction\n",
    "Multi-head attention for capturing complex relationships\n",
    "Residual connections with layer normalization\n",
    "\n",
    "## Final Optimization: DistilBERT + TextRCNN\n",
    "\n",
    "Settled on DistilBERT + TextRCNN as the optimal configuration:\n",
    "DistilBERT: Lightweight, distilled version maintaining 97% of BERT's performance\n",
    "Enhanced TextRCNN: 3 LSTM layers, 12 attention heads, improved CNN architecture\n",
    "Progressive unfreezing: BERT frozen initially, then unfrozen after 4 epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted old model file\n",
      "Using MPS (Metal GPU) device\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Delete old model file if it exists\n",
    "if os.path.exists('best_distilbert_textrcnn_model.pth'):\n",
    "    os.remove('best_distilbert_textrcnn_model.pth')\n",
    "    print(\"Deleted old model file\")\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using MPS (Metal GPU) device')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using CUDA device')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU device')\n",
    "\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved TextRCNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTextRCNN(nn.Module):\n",
    "   \n",
    "    \n",
    "    def __init__(self, hidden_size=768, num_layers=3, num_classes=2, dropout=0.3):\n",
    "        super(ImprovedTextRCNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Bidirectional LSTM with more layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size // 2,  # Bidirectional will double this\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Multi-scale CNN layers that MAINTAIN hidden_size for residual connections\n",
    "        self.conv1 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=5, padding=2)  # Keep hidden_size\n",
    "        self.conv3 = nn.Conv1d(hidden_size, hidden_size, kernel_size=7, padding=3)  # Keep hidden_size\n",
    "        \n",
    "        # Enhanced attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=12,  # More attention heads\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Improved classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.pre_attention_norm = nn.LayerNorm(hidden_size)  # Pre-attention norm\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, bert_outputs, attention_mask=None):\n",
    "        # bert_outputs: [batch_size, seq_len, hidden_size]\n",
    "        batch_size, seq_len, hidden_size = bert_outputs.shape\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        lstm_out, _ = self.lstm(bert_outputs)\n",
    "        \n",
    "        # Multi-scale CNN feature extraction\n",
    "        cnn_input = lstm_out.transpose(1, 2)\n",
    "        \n",
    "        # Apply CNN layers with residual connections\n",
    "        conv1_out = F.relu(self.conv1(cnn_input))\n",
    "        conv2_out = F.relu(self.conv2(conv1_out))\n",
    "        conv3_out = F.relu(self.conv3(conv2_out))\n",
    "        \n",
    "        # Transpose back - now cnn_out has same dimensions as lstm_out\n",
    "        cnn_out = conv3_out.transpose(1, 2)\n",
    "        \n",
    "        # Add residual connection - now dimensions match perfectly\n",
    "        cnn_out = self.layer_norm1(cnn_out + lstm_out)\n",
    "        \n",
    "        # Pre-attention normalization\n",
    "        cnn_out = self.pre_attention_norm(cnn_out)\n",
    "        \n",
    "        # Enhanced attention mechanism\n",
    "        attn_out, _ = self.attention(cnn_out, cnn_out, cnn_out)\n",
    "        \n",
    "        # Add residual connection\n",
    "        attn_out = self.layer_norm2(attn_out + cnn_out)\n",
    "        \n",
    "        # Global average pooling\n",
    "        if attention_mask is not None:\n",
    "            masked_output = attn_out * attention_mask.unsqueeze(-1)\n",
    "            pooled_output = masked_output.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "        else:\n",
    "            pooled_output = attn_out.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved DistilBERT + TextRCNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDistilBertTextRCnnClassifier(nn.Module):\n",
    "    \"\"\"Improved DistilBERT + TextRCNN classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='distilbert-base-uncased', num_classes=2, dropout=0.3):\n",
    "        super(ImprovedDistilBertTextRCnnClassifier, self).__init__()\n",
    "        \n",
    "        # DistilBERT encoder\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Improved TextRCNN classifier\n",
    "        self.textrcnn = ImprovedTextRCNN(\n",
    "            hidden_size=768,  \n",
    "            num_layers=3,     \n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Freeze BERT initially\n",
    "        self.freeze_bert()\n",
    "        \n",
    "    def freeze_bert(self):\n",
    "        \"\"\"Freeze BERT parameters.\"\"\"\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"BERT parameters frozen\")\n",
    "    \n",
    "    def unfreeze_bert(self):\n",
    "        \"\"\"Unfreeze BERT parameters for finetuning.\"\"\"\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"BERT parameters unfrozen for finetuning\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        # Pass through improved TextRCNN\n",
    "        logits = self.textrcnn(bert_outputs, attention_mask)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset(Dataset):\n",
    "    \"\"\"Custom dataset for document classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        label = item['label']\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "def load_document_pairs(data_dir):\n",
    "    \"\"\"Load document pairs directly from GitHub repository.\"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    base_url = \"https://raw.githubusercontent.com/v-gapsys/Fake-or-Real-The-Impostor-Hunt-in-Texts/main\"\n",
    "    \n",
    "    print(f\"Loading data from GitHub: {base_url}/{data_dir}\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if data_dir == 'train':\n",
    "           \n",
    "            labels_df = pd.read_csv('https://raw.githubusercontent.com/v-gapsys/Fake-or-Real-The-Impostor-Hunt-in-Texts/main/train.csv')\n",
    "            article_ids = labels_df['id'].tolist()\n",
    "        else:\n",
    "           \n",
    "            article_ids = list(range(1068))\n",
    "        \n",
    "        for article_id in article_ids:\n",
    "            if data_dir == 'train':\n",
    "                article_folder = f\"article_{str(article_id).zfill(4)}\"\n",
    "            else:\n",
    "                article_folder = f\"article_{str(article_id).zfill(4)}\"\n",
    "            \n",
    "            # Try to load file_1.txt and file_2.txt\n",
    "            file1_url = f\"{base_url}/{data_dir}/{article_folder}/file_1.txt\"\n",
    "            file2_url = f\"{base_url}/{data_dir}/{article_folder}/file_2.txt\"\n",
    "            \n",
    "            try:\n",
    "                # Load file 1\n",
    "                response1 = requests.get(file1_url)\n",
    "                if response1.status_code == 200:\n",
    "                    content1 = response1.text.strip()\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Load file 2\n",
    "                response2 = requests.get(file2_url)\n",
    "                if response2.status_code == 200:\n",
    "                    content2 = response2.text.strip()\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                pairs.append({\n",
    "                    'article_id': article_folder,\n",
    "                    'file1': 'file_1.txt',\n",
    "                    'file2': 'file_2.txt',\n",
    "                    'content1': content1,\n",
    "                    'content2': content2,\n",
    "                    'file1_path': file1_url,\n",
    "                    'file2_path': file2_url\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading article {article_folder}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully loaded {len(pairs)} document pairs from GitHub '{data_dir}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing GitHub repository: {e}\")\n",
    "      \n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def create_training_data(pairs, labels_df):\n",
    "    \"\"\"Create training data from document pairs and labels.\"\"\"\n",
    "    \n",
    "    if labels_df is None:\n",
    "        print(\"Error: No labels dataframe provided\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Creating training data with {len(labels_df)} articles...\")\n",
    "    \n",
    "    training_data = []\n",
    "    \n",
    "    for _, row in labels_df.iterrows():\n",
    "        article_id = row['id']\n",
    "        real_text_id = row['real_text_id']\n",
    "        \n",
    "        article_folder = f\"article_{str(article_id).zfill(4)}\"\n",
    "        pair = None\n",
    "        \n",
    "        for p in pairs:\n",
    "            if p['article_id'] == article_folder:\n",
    "                pair = p\n",
    "                break\n",
    "        \n",
    "        if pair is None:\n",
    "            print(f\"Warning: Could not find pair for article {article_id}\")\n",
    "            continue\n",
    "        \n",
    "        if real_text_id == 1:\n",
    "            real_content = pair['content1']\n",
    "            fake_content = pair['content2']\n",
    "        else:\n",
    "            real_content = pair['content2']\n",
    "            fake_content = pair['content1']\n",
    "        \n",
    "        training_data.append({\n",
    "            'text': real_content,\n",
    "            'label': 1,\n",
    "            'article_id': article_id,\n",
    "            'text_type': 'real'\n",
    "        })\n",
    "        \n",
    "        training_data.append({\n",
    "            'text': fake_content,\n",
    "            'label': 0,\n",
    "            'article_id': article_id,\n",
    "            'text_type': 'fake'\n",
    "        })\n",
    "    \n",
    "    print(f\"Created {len(training_data)} training examples\")\n",
    "    print(f\"    Real documents: {len([x for x in training_data if x['label'] == 1])}\")\n",
    "    print(f\"    Fake documents: {len([x for x in training_data if x['label'] == 0])}\")\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_improved(model, train_loader, val_loader, num_epochs=15, learning_rate=1e-5):\n",
    "   \n",
    "    \n",
    "    print(\"Training DistilBERT + TextRCNN model...\")\n",
    "    \n",
    "    # Loss function and optimizer with weight decay\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=0.01)\n",
    "    \n",
    "    # Improved learning rate scheduler with cosine annealing\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.15 * total_steps),  # Increased warmup\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    patience = 7  # Increased patience\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Improved gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Training Loss: {avg_loss:.4f}, Training Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}, Validation Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_distilbert_textrcnn_model.pth')\n",
    "            print(f\"   New best validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"    Early stopping after {patience} epochs without improvement\")\n",
    "            break\n",
    "        \n",
    "        # Unfreeze BERT after first few epochs for finetuning\n",
    "        if epoch == 3:  # Increased from 2\n",
    "            model.unfreeze_bert()\n",
    "            print(\"   Unfrozen BERT for finetuning\")\n",
    "    \n",
    "    print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set(model, tokenizer, test_pairs, max_length=512):\n",
    "    \"\"\"Generate predictions on test set.\"\"\"\n",
    "    \n",
    "    print(\"Generating test predictions...\")\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    for i, pair in enumerate(test_pairs):\n",
    "        article_id = pair['article_id']\n",
    "        try:\n",
    "            numeric_id = int(article_id.split('_')[1])\n",
    "            solution_id = numeric_id\n",
    "        except (IndexError, ValueError):\n",
    "            solution_id = i\n",
    "        \n",
    "        text1 = pair['content1']\n",
    "        text2 = pair['content2']\n",
    "        \n",
    "        # Tokenize texts\n",
    "        encoding1 = tokenizer(\n",
    "            text1,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        encoding2 = tokenizer(\n",
    "            text2,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids1 = encoding1['input_ids'].to(device)\n",
    "        attention_mask1 = encoding1['attention_mask'].to(device)\n",
    "        input_ids2 = encoding2['input_ids'].to(device)\n",
    "        attention_mask2 = encoding2['attention_mask'].to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "            outputs2 = model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "            \n",
    "            probs1 = F.softmax(outputs1, dim=1)\n",
    "            probs2 = F.softmax(outputs2, dim=1)\n",
    "            \n",
    "            pred1 = torch.argmax(outputs1, dim=1).item()\n",
    "            pred2 = torch.argmax(outputs2, dim=1).item()\n",
    "            \n",
    "            real_prob1 = probs1[0][1].item()\n",
    "            real_prob2 = probs2[0][1].item()\n",
    "        \n",
    "        # Determine which file is real\n",
    "        if pred1 == 1 and pred2 == 0:\n",
    "            real_text_id = 1\n",
    "        elif pred1 == 0 and pred2 == 1:\n",
    "            real_text_id = 2\n",
    "        else:\n",
    "            # Use probability\n",
    "            real_text_id = 1 if real_prob1 > real_prob2 else 2\n",
    "        \n",
    "        predictions.append({\n",
    "            'id': solution_id,\n",
    "            'real_text_id': real_text_id,\n",
    "            'text1_pred': pred1,\n",
    "            'text2_pred': pred2,\n",
    "            'text1_real_prob': real_prob1,\n",
    "            'text2_real_prob': real_prob2\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(test_pairs)} pairs...\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading data...\n",
      "Loaded 95 training articles\n",
      "Loaded 95 training pairs\n",
      "Loaded 1068 test pairs\n"
     ]
    }
   ],
   "source": [
    "# 1. Load data\n",
    "print(\"Step 1: Loading data...\")\n",
    "labels_df = pd.read_csv('train.csv')\n",
    "train_pairs = load_document_pairs('train')\n",
    "test_pairs = load_document_pairs('test')\n",
    "\n",
    "print(f\"Loaded {len(labels_df)} training articles\")\n",
    "print(f\"Loaded {len(train_pairs)} training pairs\")\n",
    "print(f\"Loaded {len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Creating training data...\n",
      "Creating training data with 95 articles...\n",
      "Created 190 training examples\n",
      "    Real documents: 95\n",
      "    Fake documents: 95\n"
     ]
    }
   ],
   "source": [
    "# 2. Create training data\n",
    "print(\"Step 2: Creating training data...\")\n",
    "train_data = create_training_data(train_pairs, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Initializing improved DistilBERT + TextRCNN...\n",
      "BERT parameters frozen\n",
      "Improved model initialized on mps\n",
      "Total parameters: 89,101,442\n",
      "Trainable parameters: 22,738,562\n"
     ]
    }
   ],
   "source": [
    "# 3. Initialize tokenizer and improved model\n",
    "print(\"Step 3: Initializing improved DistilBERT + TextRCNN...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = ImprovedDistilBertTextRCnnClassifier(\n",
    "    model_name='distilbert-base-uncased',\n",
    "    num_classes=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"Improved model initialized on {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Creating datasets...\n",
      "Training batches: 10\n",
      "Validation batches: 3\n"
     ]
    }
   ],
   "source": [
    "# 4. Create datasets and data loaders\n",
    "print(\"Step 4: Creating datasets...\")\n",
    "\n",
    "# Create DocumentDataset instances\n",
    "train_dataset = DocumentDataset(train_data, tokenizer)\n",
    "val_dataset = DocumentDataset(train_data, tokenizer)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Create data loaders with larger batch size\n",
    "batch_size = 16  # Increased from 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Training improved model...\n",
      "Training improved DistilBERT + TextRCNN model...\n",
      "Epoch 1/15:\n",
      "  Training Loss: 0.6974, Training Acc: 0.5197\n",
      "  Validation Loss: 0.7043, Validation Acc: 0.3158\n",
      "   New best validation accuracy: 0.3158\n",
      "Epoch 2/15:\n",
      "  Training Loss: 0.6922, Training Acc: 0.5592\n",
      "  Validation Loss: 0.6902, Validation Acc: 0.5263\n",
      "   New best validation accuracy: 0.5263\n",
      "Epoch 3/15:\n",
      "  Training Loss: 0.6610, Training Acc: 0.6711\n",
      "  Validation Loss: 0.6570, Validation Acc: 0.7632\n",
      "   New best validation accuracy: 0.7632\n",
      "Epoch 4/15:\n",
      "  Training Loss: 0.6236, Training Acc: 0.7500\n",
      "  Validation Loss: 0.6456, Validation Acc: 0.7368\n",
      "BERT parameters unfrozen for finetuning\n",
      "   Unfrozen BERT for finetuning\n",
      "Epoch 5/15:\n",
      "  Training Loss: 0.6085, Training Acc: 0.6908\n",
      "  Validation Loss: 0.5871, Validation Acc: 0.7632\n",
      "Epoch 6/15:\n",
      "  Training Loss: 0.5256, Training Acc: 0.7829\n",
      "  Validation Loss: 0.5338, Validation Acc: 0.7895\n",
      "   New best validation accuracy: 0.7895\n",
      "Epoch 7/15:\n",
      "  Training Loss: 0.4281, Training Acc: 0.8487\n",
      "  Validation Loss: 0.4621, Validation Acc: 0.7895\n",
      "Epoch 8/15:\n",
      "  Training Loss: 0.3627, Training Acc: 0.8487\n",
      "  Validation Loss: 0.4141, Validation Acc: 0.8158\n",
      "   New best validation accuracy: 0.8158\n",
      "Epoch 9/15:\n",
      "  Training Loss: 0.3046, Training Acc: 0.8684\n",
      "  Validation Loss: 0.3711, Validation Acc: 0.8158\n",
      "Epoch 10/15:\n",
      "  Training Loss: 0.2613, Training Acc: 0.9013\n",
      "  Validation Loss: 0.3636, Validation Acc: 0.8158\n",
      "Epoch 11/15:\n",
      "  Training Loss: 0.2445, Training Acc: 0.9079\n",
      "  Validation Loss: 0.3514, Validation Acc: 0.8158\n",
      "Epoch 12/15:\n",
      "  Training Loss: 0.2134, Training Acc: 0.9211\n",
      "  Validation Loss: 0.3653, Validation Acc: 0.8158\n",
      "Epoch 13/15:\n",
      "  Training Loss: 0.2157, Training Acc: 0.9474\n",
      "  Validation Loss: 0.3630, Validation Acc: 0.8421\n",
      "   New best validation accuracy: 0.8421\n",
      "Epoch 14/15:\n",
      "  Training Loss: 0.2124, Training Acc: 0.9408\n",
      "  Validation Loss: 0.3633, Validation Acc: 0.8158\n",
      "Epoch 15/15:\n",
      "  Training Loss: 0.1971, Training Acc: 0.9671\n",
      "  Validation Loss: 0.3631, Validation Acc: 0.8158\n",
      "\n",
      "Best validation accuracy: 0.8421\n",
      "Training completed!\n",
      "Training completed successfully! Best validation accuracy: 0.8421\n"
     ]
    }
   ],
   "source": [
    "# 5. Train the improved model\n",
    "print(\"Step 5: Training improved model...\")\n",
    "try:\n",
    "    best_val_acc = train_model_improved(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=15,\n",
    "        learning_rate=1e-5  # Lower learning rate for better stability\n",
    "    )\n",
    "    print(f\"Training completed successfully! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    best_val_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Loading best model\n",
      "Best model loaded\n"
     ]
    }
   ],
   "source": [
    "# 6. Load best model\n",
    "print(\"Step 6: Loading best model\")\n",
    "if os.path.exists('best_distilbert_textrcnn_model.pth'):\n",
    "    model.load_state_dict(torch.load('best_distilbert_textrcnn_model.pth'))\n",
    "    print(\"Best model loaded\")\n",
    "else:\n",
    "    print(\"No saved model found, using current model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: Generating test predictions...\n",
      "Generating test predictions...\n",
      "Processed 100/1068 pairs...\n",
      "Processed 200/1068 pairs...\n",
      "Processed 300/1068 pairs...\n",
      "Processed 400/1068 pairs...\n",
      "Processed 500/1068 pairs...\n",
      "Processed 600/1068 pairs...\n",
      "Processed 700/1068 pairs...\n",
      "Processed 800/1068 pairs...\n",
      "Processed 900/1068 pairs...\n",
      "Processed 1000/1068 pairs...\n"
     ]
    }
   ],
   "source": [
    "# 7. Generate predictions\n",
    "print(\"Step 7: Generating test predictions...\")\n",
    "predictions = predict_test_set(model, tokenizer, test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved solution file saved as: Hunt_In_Text_Solution_Improved.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# 8. Create solution file\n",
    "solution_df = pd.DataFrame(predictions)\n",
    "submission_df = solution_df[['id', 'real_text_id']].copy()\n",
    "\n",
    "submission_df = submission_df.sort_values('id').reset_index(drop=True)\n",
    "submission_df['id'] = submission_df['id'].astype(int)\n",
    "submission_df['real_text_id'] = submission_df['real_text_id'].astype(int)\n",
    "\n",
    "solution_file = 'Hunt_In_Text_Solution_Improved.csv'\n",
    "submission_df.to_csv(solution_file, index=False)\n",
    "\n",
    "print(f\"Improved solution file saved as: {solution_file}\")\n",
    "print(\"Pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
