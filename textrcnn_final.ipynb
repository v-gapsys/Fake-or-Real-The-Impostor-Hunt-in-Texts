{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved DistilBERT + TextRCNN Pipeline\n",
    "## Advanced Document Classification with Optimized Parameters\n",
    "\n",
    "This notebook implements an improved DistilBERT + TextRCNN architecture for document authenticity classification with better parameters and architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development Journey: From Traditional ML to Advanced Neural Architectures\n",
    "\n",
    "## Initial Approach: Traditional Machine Learning\n",
    "Started with TF-IDF + XGBoost as a baseline approach. This traditional pipeline used:\n",
    "TF-IDF vectorization for text feature extraction\n",
    "XGBoost classifier for document authenticity classification\n",
    "Basic cross-validation and hyperparameter tuning\n",
    "\n",
    "## First Neural Approach: BERT\n",
    "\n",
    "Moved to BERT (Bidirectional Encoder Representations from Transformers) as the primary model:\n",
    "Standard BERT for contextual text understanding\n",
    "Fine-tuning on the document authenticity task\n",
    "Pooled output classification with custom classification heads\n",
    "\n",
    "## Domain-Specific Exploration: SciBERT\n",
    "\n",
    "Experimented with SciBERT (scientific BERT) to leverage domain-specific knowledge:\n",
    "Scientific vocabulary pre-training for better understanding of technical documents\n",
    "Specialized tokenization for scientific and engineering terminology\n",
    "Full fine-tuning approach with unfrozen BERT layers\n",
    "\n",
    "## Ensemble and Pseudo-Labeling Breakthrough\n",
    "\n",
    "Implemented pseudo-labeling techniques that significantly improved results:\n",
    "Random Forest pseudo-labeling for fast, reliable expansion of training data\n",
    "BERT training on expanded dataset using high-confidence predictions\n",
    "\n",
    "## Advanced Architecture: TextRCNN\n",
    "\n",
    "Developed TextRCNN (Text RNN + CNN) architecture combining:\n",
    "Bidirectional LSTM for sequential text processing\n",
    "Multi-scale CNN (3x3, 5x5, 7x7 kernels) for local feature extraction\n",
    "Multi-head attention for capturing complex relationships\n",
    "Residual connections with layer normalization\n",
    "\n",
    "## Final Optimization: DistilBERT + TextRCNN\n",
    "\n",
    "Settled on DistilBERT + TextRCNN as the optimal configuration:\n",
    "DistilBERT: Lightweight, distilled version maintaining 97% of BERT's performance\n",
    "Enhanced TextRCNN: 3 LSTM layers, 12 attention heads, improved CNN architecture\n",
    "Progressive unfreezing: BERT frozen initially, then unfrozen after 4 epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal GPU) device\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Delete old model file if it exists\n",
    "if os.path.exists('best_distilbert_textrcnn_model.pth'):\n",
    "    os.remove('best_distilbert_textrcnn_model.pth')\n",
    "    print(\"Deleted old model file\")\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using MPS (Metal GPU) device')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using CUDA device')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU device')\n",
    "\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved TextRCNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTextRCNN(nn.Module):\n",
    "   \n",
    "    \n",
    "    def __init__(self, hidden_size=768, num_layers=3, num_classes=2, dropout=0.3):\n",
    "        super(ImprovedTextRCNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Bidirectional LSTM with more layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size // 2,  # Bidirectional will double this\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Multi-scale CNN layers that MAINTAIN hidden_size for residual connections\n",
    "        self.conv1 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=5, padding=2)  # Keep hidden_size\n",
    "        self.conv3 = nn.Conv1d(hidden_size, hidden_size, kernel_size=7, padding=3)  # Keep hidden_size\n",
    "        \n",
    "        # Enhanced attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=12,  # More attention heads\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Improved classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.pre_attention_norm = nn.LayerNorm(hidden_size)  # Pre-attention norm\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, bert_outputs, attention_mask=None):\n",
    "        # bert_outputs: [batch_size, seq_len, hidden_size]\n",
    "        batch_size, seq_len, hidden_size = bert_outputs.shape\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        lstm_out, _ = self.lstm(bert_outputs)\n",
    "        \n",
    "        # Multi-scale CNN feature extraction\n",
    "        cnn_input = lstm_out.transpose(1, 2)\n",
    "        \n",
    "        # Apply CNN layers with residual connections\n",
    "        conv1_out = F.relu(self.conv1(cnn_input))\n",
    "        conv2_out = F.relu(self.conv2(conv1_out))\n",
    "        conv3_out = F.relu(self.conv3(conv2_out))\n",
    "        \n",
    "        # Transpose back - now cnn_out has same dimensions as lstm_out\n",
    "        cnn_out = conv3_out.transpose(1, 2)\n",
    "        \n",
    "        # Add residual connection - now dimensions match perfectly\n",
    "        cnn_out = self.layer_norm1(cnn_out + lstm_out)\n",
    "        \n",
    "        # Pre-attention normalization\n",
    "        cnn_out = self.pre_attention_norm(cnn_out)\n",
    "        \n",
    "        # Enhanced attention mechanism\n",
    "        attn_out, _ = self.attention(cnn_out, cnn_out, cnn_out)\n",
    "        \n",
    "        # Add residual connection\n",
    "        attn_out = self.layer_norm2(attn_out + cnn_out)\n",
    "        \n",
    "        # Global average pooling\n",
    "        if attention_mask is not None:\n",
    "            masked_output = attn_out * attention_mask.unsqueeze(-1)\n",
    "            pooled_output = masked_output.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "        else:\n",
    "            pooled_output = attn_out.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved DistilBERT + TextRCNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDistilBertTextRCnnClassifier(nn.Module):\n",
    "    \"\"\"Improved DistilBERT + TextRCNN classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='distilbert-base-uncased', num_classes=2, dropout=0.3):\n",
    "        super(ImprovedDistilBertTextRCnnClassifier, self).__init__()\n",
    "        \n",
    "        # DistilBERT encoder\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Improved TextRCNN classifier\n",
    "        self.textrcnn = ImprovedTextRCNN(\n",
    "            hidden_size=768,  \n",
    "            num_layers=3,     \n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Freeze BERT initially\n",
    "        self.freeze_bert()\n",
    "        \n",
    "    def freeze_bert(self):\n",
    "        \"\"\"Freeze BERT parameters.\"\"\"\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"BERT parameters frozen\")\n",
    "    \n",
    "    def unfreeze_bert(self):\n",
    "        \"\"\"Unfreeze BERT parameters for finetuning.\"\"\"\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"BERT parameters unfrozen for finetuning\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        # Pass through improved TextRCNN\n",
    "        logits = self.textrcnn(bert_outputs, attention_mask)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset(Dataset):\n",
    "    \"\"\"Custom dataset for document classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        label = item['label']\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_pairs(data_dir):\n",
    "    \"\"\"Load document pairs from local directory.\"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"{data_dir} directory not found!\")\n",
    "        print(\"Make sure the data directories are in your current folder\")\n",
    "        return []\n",
    "    \n",
    "    for article_dir in sorted(os.listdir(data_dir)):\n",
    "        article_path = os.path.join(data_dir, article_dir)\n",
    "        if os.path.isdir(article_path):\n",
    "            files = [f for f in os.listdir(article_path) if f.endswith('.txt')]\n",
    "            if len(files) == 2:\n",
    "                with open(os.path.join(article_path, files[0]), 'r', encoding='utf8') as f:\n",
    "                    content1 = f.read().strip()\n",
    "                with open(os.path.join(article_path, files[1]), 'r', encoding='utf8') as f:\n",
    "                    content2 = f.read().strip()\n",
    "                \n",
    "                pairs.append({\n",
    "                    'article_id': article_dir,\n",
    "                    'content1': content1,\n",
    "                    'content2': content2\n",
    "                })\n",
    "    \n",
    "    print(f\"Loaded {len(pairs)} pairs from {data_dir}\")\n",
    "    return pairs\n",
    "\n",
    "def create_training_data(pairs, labels_df):\n",
    "    \"\"\"Create training data from document pairs and labels.\"\"\"\n",
    "    \n",
    "    if labels_df is None:\n",
    "        print(\"Error: No labels dataframe provided\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Creating training data with {len(labels_df)} articles...\")\n",
    "    \n",
    "    training_data = []\n",
    "    \n",
    "    for _, row in labels_df.iterrows():\n",
    "        article_id = row['id']\n",
    "        real_text_id = row['real_text_id']\n",
    "        \n",
    "        article_folder = f\"article_{str(article_id).zfill(4)}\"\n",
    "        pair = None\n",
    "        \n",
    "        for p in pairs:\n",
    "            if p['article_id'] == article_folder:\n",
    "                pair = p\n",
    "                break\n",
    "        \n",
    "        if pair is None:\n",
    "            print(f\"Warning: Could not find pair for article {article_id}\")\n",
    "            continue\n",
    "        \n",
    "        if real_text_id == 1:\n",
    "            real_content = pair['content1']\n",
    "            fake_content = pair['content2']\n",
    "        else:\n",
    "            real_content = pair['content2']\n",
    "            fake_content = pair['content1']\n",
    "        \n",
    "        training_data.append({\n",
    "            'text': real_content,\n",
    "            'label': 1,\n",
    "            'article_id': article_id,\n",
    "            'text_type': 'real'\n",
    "        })\n",
    "        \n",
    "        training_data.append({\n",
    "            'text': fake_content,\n",
    "            'label': 0,\n",
    "            'article_id': article_id,\n",
    "            'text_type': 'fake'\n",
    "        })\n",
    "    \n",
    "    print(f\"Created {len(training_data)} training examples\")\n",
    "    print(f\"Real documents: {len([x for x in training_data if x['label'] == 1])}\")\n",
    "    print(f\"Fake documents: {len([x for x in training_data if x['label'] == 0])}\")\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_improved(model, train_loader, val_loader, num_epochs=15, learning_rate=1e-5):\n",
    "   \n",
    "    \n",
    "    print(\"Training DistilBERT + TextRCNN model...\")\n",
    "    \n",
    "    # Loss function and optimizer with weight decay\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=0.01)\n",
    "    \n",
    "    # Improved learning rate scheduler with cosine annealing\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.15 * total_steps),  # Increased warmup\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    patience = 7  # Increased patience\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Improved gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Training Loss: {avg_loss:.4f}, Training Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}, Validation Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_distilbert_textrcnn_model.pth')\n",
    "            print(f\"   New best validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"    Early stopping after {patience} epochs without improvement\")\n",
    "            break\n",
    "        \n",
    "        # Unfreeze BERT after first few epochs for finetuning\n",
    "        if epoch == 3:  # Increased from 2\n",
    "            model.unfreeze_bert()\n",
    "            print(\"   Unfrozen BERT for finetuning\")\n",
    "    \n",
    "    print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set(model, tokenizer, test_pairs, max_length=512):\n",
    "    \"\"\"Generate predictions on test set.\"\"\"\n",
    "    \n",
    "    print(\"Generating test predictions...\")\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    for i, pair in enumerate(test_pairs):\n",
    "        article_id = pair['article_id']\n",
    "        try:\n",
    "            numeric_id = int(article_id.split('_')[1])\n",
    "            solution_id = numeric_id\n",
    "        except (IndexError, ValueError):\n",
    "            solution_id = i\n",
    "        \n",
    "        text1 = pair['content1']\n",
    "        text2 = pair['content2']\n",
    "        \n",
    "        # Tokenize texts\n",
    "        encoding1 = tokenizer(\n",
    "            text1,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        encoding2 = tokenizer(\n",
    "            text2,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids1 = encoding1['input_ids'].to(device)\n",
    "        attention_mask1 = encoding1['attention_mask'].to(device)\n",
    "        input_ids2 = encoding2['input_ids'].to(device)\n",
    "        attention_mask2 = encoding2['attention_mask'].to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "            outputs2 = model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "            \n",
    "            probs1 = F.softmax(outputs1, dim=1)\n",
    "            probs2 = F.softmax(outputs2, dim=1)\n",
    "            \n",
    "            pred1 = torch.argmax(outputs1, dim=1).item()\n",
    "            pred2 = torch.argmax(outputs2, dim=1).item()\n",
    "            \n",
    "            real_prob1 = probs1[0][1].item()\n",
    "            real_prob2 = probs2[0][1].item()\n",
    "        \n",
    "        # Determine which file is real\n",
    "        if pred1 == 1 and pred2 == 0:\n",
    "            real_text_id = 1\n",
    "        elif pred1 == 0 and pred2 == 1:\n",
    "            real_text_id = 2\n",
    "        else:\n",
    "            # Use probability\n",
    "            real_text_id = 1 if real_prob1 > real_prob2 else 2\n",
    "        \n",
    "        predictions.append({\n",
    "            'id': solution_id,\n",
    "            'real_text_id': real_text_id,\n",
    "            'text1_pred': pred1,\n",
    "            'text2_pred': pred2,\n",
    "            'text1_real_prob': real_prob1,\n",
    "            'text2_real_prob': real_prob2\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(test_pairs)} pairs...\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading data from local directories...\n",
      "Loaded training labels: 95 articles\n",
      "Loading document pairs...\n",
      "Loaded 95 pairs from train\n",
      "Loaded 1068 pairs from test\n",
      "Data Summary:\n",
      "Training articles: 95\n",
      "Training pairs: 95\n",
      "Test pairs: 1068\n"
     ]
    }
   ],
   "source": [
    "# 1. Load data from local directories\n",
    "print(\"Step 1: Loading data from local directories...\")\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    labels_df = pd.read_csv('train.csv')\n",
    "    print(f\"Loaded training labels: {len(labels_df)} articles\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading train.csv: {e}\")\n",
    "    print(\"Make sure train.csv is in your current directory\")\n",
    "    labels_df = None\n",
    "\n",
    "# Load document pairs\n",
    "print(\"Loading document pairs...\")\n",
    "train_pairs = load_document_pairs('train')\n",
    "test_pairs = load_document_pairs('test')\n",
    "\n",
    "print(f\"Data Summary:\")\n",
    "print(f\"Training articles: {len(labels_df) if labels_df is not None else 'N/A'}\")\n",
    "print(f\"Training pairs: {len(train_pairs)}\")\n",
    "print(f\"Test pairs: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Creating training data...\n",
      "Creating training data with 95 articles...\n",
      "Created 190 training examples\n",
      "Real documents: 95\n",
      "Fake documents: 95\n"
     ]
    }
   ],
   "source": [
    "# 2. Create training data\n",
    "print(\"Step 2: Creating training data...\")\n",
    "train_data = create_training_data(train_pairs, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Initializing improved DistilBERT + TextRCNN...\n",
      "BERT parameters frozen\n",
      "Improved model initialized on mps\n",
      "Total parameters: 89,101,442\n",
      "Trainable parameters: 22,738,562\n"
     ]
    }
   ],
   "source": [
    "# 3. Initialize tokenizer and improved model\n",
    "print(\"Step 3: Initializing improved DistilBERT + TextRCNN...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = ImprovedDistilBertTextRCnnClassifier(\n",
    "    model_name='distilbert-base-uncased',\n",
    "    num_classes=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"Improved model initialized on {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Creating datasets...\n",
      "Training batches: 10\n",
      "Validation batches: 3\n"
     ]
    }
   ],
   "source": [
    "# 4. Create datasets and data loaders\n",
    "print(\"Step 4: Creating datasets...\")\n",
    "\n",
    "# Create DocumentDataset instances\n",
    "train_dataset = DocumentDataset(train_data, tokenizer)\n",
    "val_dataset = DocumentDataset(train_data, tokenizer)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Create data loaders with larger batch size\n",
    "batch_size = 16  # Increased from 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Training improved model...\n",
      "Training DistilBERT + TextRCNN model...\n",
      "Epoch 1/15:\n",
      "  Training Loss: 0.6875, Training Acc: 0.5132\n",
      "  Validation Loss: 0.6911, Validation Acc: 0.4474\n",
      "   New best validation accuracy: 0.4474\n",
      "Epoch 2/15:\n",
      "  Training Loss: 0.6769, Training Acc: 0.5526\n",
      "  Validation Loss: 0.6742, Validation Acc: 0.7632\n",
      "   New best validation accuracy: 0.7632\n",
      "Epoch 3/15:\n",
      "  Training Loss: 0.6550, Training Acc: 0.7237\n",
      "  Validation Loss: 0.6489, Validation Acc: 0.7895\n",
      "   New best validation accuracy: 0.7895\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 5: Training improved model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     best_val_acc = \u001b[43mtrain_model_improved\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Lower learning rate for better stability\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining completed successfully! Best validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain_model_improved\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[39m\n\u001b[32m     43\u001b[39m optimizer.step()\n\u001b[32m     44\u001b[39m scheduler.step()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n\u001b[32m     48\u001b[39m total += labels.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 5. Train the improved model\n",
    "print(\"Step 5: Training improved model...\")\n",
    "try:\n",
    "    best_val_acc = train_model_improved(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=15,\n",
    "        learning_rate=1e-5  # Lower learning rate for better stability\n",
    "    )\n",
    "    print(f\"Training completed successfully! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    best_val_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Load best model\n",
    "print(\"Step 6: Loading best model\")\n",
    "if os.path.exists('best_distilbert_textrcnn_model.pth'):\n",
    "    model.load_state_dict(torch.load('best_distilbert_textrcnn_model.pth'))\n",
    "    print(\"Best model loaded\")\n",
    "else:\n",
    "    print(\"No saved model found, using current model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Generate predictions\n",
    "print(\"Step 7: Generating test predictions...\")\n",
    "predictions = predict_test_set(model, tokenizer, test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create solution file\n",
    "solution_df = pd.DataFrame(predictions)\n",
    "submission_df = solution_df[['id', 'real_text_id']].copy()\n",
    "\n",
    "submission_df = submission_df.sort_values('id').reset_index(drop=True)\n",
    "submission_df['id'] = submission_df['id'].astype(int)\n",
    "submission_df['real_text_id'] = submission_df['real_text_id'].astype(int)\n",
    "\n",
    "solution_file = 'Hunt_In_Text_Solution_Improved.csv'\n",
    "submission_df.to_csv(solution_file, index=False)\n",
    "\n",
    "print(f\"Improved solution file saved as: {solution_file}\")\n",
    "print(\"Pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
